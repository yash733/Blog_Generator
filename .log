2025-03-19 19:37:14,280 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2025-03-19 19:37:14,280 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-03-19 19:37:14,280 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-03-19 19:37:15,121 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 200 0
2025-03-19 19:37:15,363 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 200 0
2025-03-19 19:37:15,592 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 200 0
2025-03-19 19:37:15,858 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 200 0
2025-03-19 19:37:16,092 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 200 0
2025-03-19 19:37:16,332 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2025-03-19 19:37:16,573 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 200 0
2025-03-19 19:37:17,058 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2025-03-19 19:37:17,369 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/revision/main HTTP/1.1" 200 6729
2025-03-19 19:37:17,619 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6729
2025-03-19 19:37:18,102 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.smith.langchain.com:443
2025-03-19 19:37:18,164 - Error/Exception - INFO - Topic - Tool Node
2025-03-19 19:37:18,164 - groq._base_client - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'Generate a indepth plan for the blog'}, {'role': 'user', 'content': 'Here is the topic: Unsupervised Learning, meta data from user: explain the core functioning.'}], 'model': 'llama3-70b-8192', 'n': 1, 'stop': None, 'stream': False, 'temperature': 0.7, 'tool_choice': {'type': 'function', 'function': {'name': 'Blog'}}, 'tools': [{'type': 'function', 'function': {'name': 'Blog', 'description': '', 'parameters': {'properties': {'sections': {'description': 'Holdes list of Subheadings with their discription', 'items': {'properties': {'name': {'description': 'Subheading of the blog', 'type': 'string'}, 'description': {'description': 'Provide a short description of subheading', 'type': 'string'}}, 'required': ['name', 'description'], 'type': 'object'}, 'type': 'array'}}, 'required': ['sections'], 'type': 'object'}}}]}}
2025-03-19 19:37:18,164 - groq._base_client - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2025-03-19 19:37:18,164 - httpcore.connection - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=None socket_options=None
2025-03-19 19:37:18,180 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002151F5640E0>
2025-03-19 19:37:18,180 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002151EE99E50> server_hostname='api.groq.com' timeout=None
2025-03-19 19:37:18,196 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002151F53FE00>
2025-03-19 19:37:18,196 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-03-19 19:37:18,196 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-03-19 19:37:18,196 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-03-19 19:37:18,196 - httpcore.http11 - DEBUG - send_request_body.complete
2025-03-19 19:37:18,196 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-03-19 19:37:18,431 - urllib3.connectionpool - DEBUG - https://api.smith.langchain.com:443 "GET /info HTTP/1.1" 200 672
2025-03-19 19:37:18,510 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.smith.langchain.com:443
2025-03-19 19:37:18,833 - urllib3.connectionpool - DEBUG - https://api.smith.langchain.com:443 "POST /runs/multipart HTTP/1.1" 202 34
2025-03-19 19:37:19,246 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 19 Mar 2025 14:07:20 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'Vary', b'Origin'), (b'X-Groq-Region', b'gcp-asia-south1'), (b'X-Ratelimit-Limit-Requests', b'14400'), (b'X-Ratelimit-Limit-Tokens', b'6000'), (b'X-Ratelimit-Remaining-Requests', b'14399'), (b'X-Ratelimit-Remaining-Tokens', b'5958'), (b'X-Ratelimit-Reset-Requests', b'6s'), (b'X-Ratelimit-Reset-Tokens', b'420ms'), (b'X-Request-Id', b'req_01jpqbnsjtf19bfzp063cb42sp'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=Pg6dFkZPowKNyrkny9BflYUshJheBkl2IbOG9WRDYQk-1742393240-1.0.1.1-5bUZ2C2XKfkc8t1704ybpKy9vmHau3T5W1rqIWPQ1hBHF.NAdjSZY1XC51cc.Z7d_CS3rmi0lQflidul255ZRxuzlGzjKRmMiFFNDYPw4Vc; path=/; expires=Wed, 19-Mar-25 14:37:20 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'922d890fbf34032c-AMD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-03-19 19:37:19,246 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-03-19 19:37:19,246 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-03-19 19:37:19,246 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-03-19 19:37:19,246 - httpcore.http11 - DEBUG - response_closed.started
2025-03-19 19:37:19,246 - httpcore.http11 - DEBUG - response_closed.complete
2025-03-19 19:37:19,246 - groq._base_client - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "200 OK" Headers({'date': 'Wed, 19 Mar 2025 14:07:20 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-groq-region': 'gcp-asia-south1', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '6000', 'x-ratelimit-remaining-requests': '14399', 'x-ratelimit-remaining-tokens': '5958', 'x-ratelimit-reset-requests': '6s', 'x-ratelimit-reset-tokens': '420ms', 'x-request-id': 'req_01jpqbnsjtf19bfzp063cb42sp', 'cf-cache-status': 'DYNAMIC', 'set-cookie': '__cf_bm=Pg6dFkZPowKNyrkny9BflYUshJheBkl2IbOG9WRDYQk-1742393240-1.0.1.1-5bUZ2C2XKfkc8t1704ybpKy9vmHau3T5W1rqIWPQ1hBHF.NAdjSZY1XC51cc.Z7d_CS3rmi0lQflidul255ZRxuzlGzjKRmMiFFNDYPw4Vc; path=/; expires=Wed, 19-Mar-25 14:37:20 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None', 'server': 'cloudflare', 'cf-ray': '922d890fbf34032c-AMD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-03-19 19:37:19,261 - Status - INFO - Topic State
2025-03-19 19:37:19,261 - Status - INFO - API Call Parallel
2025-03-19 19:37:19,261 - groq._base_client - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'Write a section of blog, with the help of provided section header and its description. Structure the content sutch that its easily understandable by any person.'}, {'role': 'user', 'content': "Here is blog section header: Introduction, and section description: Unsupervised Learning is a type of machine learning where the model learns from unlabeled data. It's called unsupervised because there is no correct output or response variable to guide the learning process.. User adition data explain the core functioning., provide answer in makrdown format."}], 'model': 'llama3-70b-8192', 'n': 1, 'stop': None, 'stream': False, 'temperature': 0.7}}
2025-03-19 19:37:19,261 - groq._base_client - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2025-03-19 19:37:19,261 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-03-19 19:37:19,261 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-03-19 19:37:19,261 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-03-19 19:37:19,261 - httpcore.http11 - DEBUG - send_request_body.complete
2025-03-19 19:37:19,261 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-03-19 19:37:19,261 - groq._base_client - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'Write a section of blog, with the help of provided section header and its description. Structure the content sutch that its easily understandable by any person.'}, {'role': 'user', 'content': 'Here is blog section header: Types of Unsupervised Learning, and section description: There are two main types of unsupervised learning: clustering and dimensionality reduction.. User adition data explain the core functioning., provide answer in makrdown format.'}], 'model': 'llama3-70b-8192', 'n': 1, 'stop': None, 'stream': False, 'temperature': 0.7}}
2025-03-19 19:37:19,261 - groq._base_client - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2025-03-19 19:37:19,261 - httpcore.connection - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=None socket_options=None
2025-03-19 19:37:19,261 - groq._base_client - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'Write a section of blog, with the help of provided section header and its description. Structure the content sutch that its easily understandable by any person.'}, {'role': 'user', 'content': 'Here is blog section header: Clustering, and section description: Clustering is a type of unsupervised learning where the model groups similar data points into clusters based on their characteristics.. User adition data explain the core functioning., provide answer in makrdown format.'}], 'model': 'llama3-70b-8192', 'n': 1, 'stop': None, 'stream': False, 'temperature': 0.7}}
2025-03-19 19:37:19,277 - groq._base_client - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2025-03-19 19:37:19,277 - httpcore.connection - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=None socket_options=None
2025-03-19 19:37:19,277 - groq._base_client - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'Write a section of blog, with the help of provided section header and its description. Structure the content sutch that its easily understandable by any person.'}, {'role': 'user', 'content': 'Here is blog section header: Dimensionality Reduction, and section description: Dimensionality reduction is a type of unsupervised learning where the model reduces the number of features or variables in the data, while retaining the most important information.. User adition data explain the core functioning., provide answer in makrdown format.'}], 'model': 'llama3-70b-8192', 'n': 1, 'stop': None, 'stream': False, 'temperature': 0.7}}
2025-03-19 19:37:19,277 - groq._base_client - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2025-03-19 19:37:19,277 - httpcore.connection - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=None socket_options=None
2025-03-19 19:37:19,277 - groq._base_client - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'Write a section of blog, with the help of provided section header and its description. Structure the content sutch that its easily understandable by any person.'}, {'role': 'user', 'content': 'Here is blog section header: Core Functioning, and section description: The core functioning of unsupervised learning involves identifying patterns or structure in the data, without prior knowledge of the expected outcome.. User adition data explain the core functioning., provide answer in makrdown format.'}], 'model': 'llama3-70b-8192', 'n': 1, 'stop': None, 'stream': False, 'temperature': 0.7}}
2025-03-19 19:37:19,277 - groq._base_client - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2025-03-19 19:37:19,277 - httpcore.connection - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=None socket_options=None
2025-03-19 19:37:19,277 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002151F5AE9F0>
2025-03-19 19:37:19,277 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002151EE99E50> server_hostname='api.groq.com' timeout=None
2025-03-19 19:37:19,277 - groq._base_client - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'Write a section of blog, with the help of provided section header and its description. Structure the content sutch that its easily understandable by any person.'}, {'role': 'user', 'content': 'Here is blog section header: Real-world Applications, and section description: Unsupervised learning has many real-world applications, including customer segmentation, anomaly detection, and recommender systems.. User adition data explain the core functioning., provide answer in makrdown format.'}], 'model': 'llama3-70b-8192', 'n': 1, 'stop': None, 'stream': False, 'temperature': 0.7}}
2025-03-19 19:37:19,277 - groq._base_client - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2025-03-19 19:37:19,277 - httpcore.connection - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=None socket_options=None
2025-03-19 19:37:19,277 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002151F5C1A30>
2025-03-19 19:37:19,277 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002151EE99E50> server_hostname='api.groq.com' timeout=None
2025-03-19 19:37:19,277 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002151F5C03B0>
2025-03-19 19:37:19,277 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002151EE99E50> server_hostname='api.groq.com' timeout=None
2025-03-19 19:37:19,277 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002151F5AEC90>
2025-03-19 19:37:19,277 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002151EE99E50> server_hostname='api.groq.com' timeout=None
2025-03-19 19:37:19,277 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002151F5C2AB0>
2025-03-19 19:37:19,277 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002151EE99E50> server_hostname='api.groq.com' timeout=None
2025-03-19 19:37:19,293 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002151F5AE5A0>
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - send_request_body.complete
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-03-19 19:37:19,293 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002151F5C15E0>
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - send_request_body.complete
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-03-19 19:37:19,293 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002151F5AFDA0>
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-03-19 19:37:19,293 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002151F5C2930>
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-03-19 19:37:19,293 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002151F5C3FB0>
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - send_request_body.complete
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - send_request_body.complete
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - send_request_body.complete
2025-03-19 19:37:19,293 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-03-19 19:37:20,162 - urllib3.connectionpool - DEBUG - https://api.smith.langchain.com:443 "POST /runs/multipart HTTP/1.1" 202 34
2025-03-19 19:37:20,444 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 19 Mar 2025 14:07:21 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'Vary', b'Origin'), (b'X-Groq-Region', b'gcp-asia-south1'), (b'X-Ratelimit-Limit-Requests', b'14400'), (b'X-Ratelimit-Limit-Tokens', b'6000'), (b'X-Ratelimit-Remaining-Requests', b'14398'), (b'X-Ratelimit-Remaining-Tokens', b'4654'), (b'X-Ratelimit-Reset-Requests', b'11.023999999s'), (b'X-Ratelimit-Reset-Tokens', b'13.451999999s'), (b'X-Request-Id', b'req_01jpqbnthaf19byjx54nf56dt4'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'922d89166b10032c-AMD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-03-19 19:37:20,444 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-03-19 19:37:20,444 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-03-19 19:37:20,444 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-03-19 19:37:20,444 - httpcore.http11 - DEBUG - response_closed.started
2025-03-19 19:37:20,444 - httpcore.http11 - DEBUG - response_closed.complete
2025-03-19 19:37:20,444 - groq._base_client - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "200 OK" Headers({'date': 'Wed, 19 Mar 2025 14:07:21 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-groq-region': 'gcp-asia-south1', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '6000', 'x-ratelimit-remaining-requests': '14398', 'x-ratelimit-remaining-tokens': '4654', 'x-ratelimit-reset-requests': '11.023999999s', 'x-ratelimit-reset-tokens': '13.451999999s', 'x-request-id': 'req_01jpqbnthaf19byjx54nf56dt4', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '922d89166b10032c-AMD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-03-19 19:37:20,459 - Status - INFO - LLM call
2025-03-19 19:37:20,475 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 19 Mar 2025 14:07:21 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'Vary', b'Origin'), (b'X-Groq-Region', b'gcp-asia-south1'), (b'X-Ratelimit-Limit-Requests', b'14400'), (b'X-Ratelimit-Limit-Tokens', b'6000'), (b'X-Ratelimit-Remaining-Requests', b'14394'), (b'X-Ratelimit-Remaining-Tokens', b'4665'), (b'X-Ratelimit-Reset-Requests', b'35.995999999s'), (b'X-Ratelimit-Reset-Tokens', b'13.343s'), (b'X-Request-Id', b'req_01jpqbntkgf198asjmwbtkb3gx'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'922d8916aeeb0330-AMD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-03-19 19:37:20,475 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-03-19 19:37:20,475 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-03-19 19:37:20,475 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-03-19 19:37:20,475 - httpcore.http11 - DEBUG - response_closed.started
2025-03-19 19:37:20,475 - httpcore.http11 - DEBUG - response_closed.complete
2025-03-19 19:37:20,475 - groq._base_client - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "200 OK" Headers({'date': 'Wed, 19 Mar 2025 14:07:21 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-groq-region': 'gcp-asia-south1', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '6000', 'x-ratelimit-remaining-requests': '14394', 'x-ratelimit-remaining-tokens': '4665', 'x-ratelimit-reset-requests': '35.995999999s', 'x-ratelimit-reset-tokens': '13.343s', 'x-request-id': 'req_01jpqbntkgf198asjmwbtkb3gx', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '922d8916aeeb0330-AMD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-03-19 19:37:20,491 - Status - INFO - LLM call
2025-03-19 19:37:20,603 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 19 Mar 2025 14:07:21 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'Vary', b'Origin'), (b'X-Groq-Region', b'gcp-asia-south1'), (b'X-Ratelimit-Limit-Requests', b'14400'), (b'X-Ratelimit-Limit-Tokens', b'6000'), (b'X-Ratelimit-Remaining-Requests', b'14397'), (b'X-Ratelimit-Remaining-Tokens', b'4675'), (b'X-Ratelimit-Reset-Requests', b'17.952s'), (b'X-Ratelimit-Reset-Tokens', b'13.243999999s'), (b'X-Request-Id', b'req_01jpqbntjvfvg9wngthww2knaw'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'922d8916acb50333-AMD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-03-19 19:37:20,604 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-03-19 19:37:20,604 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-03-19 19:37:20,605 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-03-19 19:37:20,606 - httpcore.http11 - DEBUG - response_closed.started
2025-03-19 19:37:20,606 - httpcore.http11 - DEBUG - response_closed.complete
2025-03-19 19:37:20,607 - groq._base_client - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "200 OK" Headers({'date': 'Wed, 19 Mar 2025 14:07:21 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-groq-region': 'gcp-asia-south1', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '6000', 'x-ratelimit-remaining-requests': '14397', 'x-ratelimit-remaining-tokens': '4675', 'x-ratelimit-reset-requests': '17.952s', 'x-ratelimit-reset-tokens': '13.243999999s', 'x-request-id': 'req_01jpqbntjvfvg9wngthww2knaw', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '922d8916acb50333-AMD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-03-19 19:37:20,612 - Status - INFO - LLM call
2025-03-19 19:37:20,640 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 19 Mar 2025 14:07:21 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'Vary', b'Origin'), (b'X-Groq-Region', b'gcp-asia-south1'), (b'X-Ratelimit-Limit-Requests', b'14400'), (b'X-Ratelimit-Limit-Tokens', b'6000'), (b'X-Ratelimit-Remaining-Requests', b'14393'), (b'X-Ratelimit-Remaining-Tokens', b'4687'), (b'X-Ratelimit-Reset-Requests', b'41.990999999s'), (b'X-Ratelimit-Reset-Tokens', b'13.123s'), (b'X-Request-Id', b'req_01jpqbntktfvgby091979g11ay'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'922d89169efc032f-AMD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-03-19 19:37:20,640 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-03-19 19:37:20,640 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-03-19 19:37:20,640 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-03-19 19:37:20,640 - httpcore.http11 - DEBUG - response_closed.started
2025-03-19 19:37:20,640 - httpcore.http11 - DEBUG - response_closed.complete
2025-03-19 19:37:20,640 - groq._base_client - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "200 OK" Headers({'date': 'Wed, 19 Mar 2025 14:07:21 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-groq-region': 'gcp-asia-south1', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '6000', 'x-ratelimit-remaining-requests': '14393', 'x-ratelimit-remaining-tokens': '4687', 'x-ratelimit-reset-requests': '41.990999999s', 'x-ratelimit-reset-tokens': '13.123s', 'x-request-id': 'req_01jpqbntktfvgby091979g11ay', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '922d89169efc032f-AMD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-03-19 19:37:20,650 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 19 Mar 2025 14:07:21 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'Vary', b'Origin'), (b'X-Groq-Region', b'gcp-asia-south1'), (b'X-Ratelimit-Limit-Requests', b'14400'), (b'X-Ratelimit-Limit-Tokens', b'6000'), (b'X-Ratelimit-Remaining-Requests', b'14396'), (b'X-Ratelimit-Remaining-Tokens', b'4675'), (b'X-Ratelimit-Reset-Requests', b'23.984999999s'), (b'X-Ratelimit-Reset-Tokens', b'13.248s'), (b'X-Request-Id', b'req_01jpqbntkafvga1fwcq0zqjha5'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'922d8916acb30333-AMD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-03-19 19:37:20,650 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-03-19 19:37:20,650 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-03-19 19:37:20,650 - Status - INFO - LLM call
2025-03-19 19:37:20,657 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-03-19 19:37:20,657 - httpcore.http11 - DEBUG - response_closed.started
2025-03-19 19:37:20,657 - httpcore.http11 - DEBUG - response_closed.complete
2025-03-19 19:37:20,657 - groq._base_client - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "200 OK" Headers({'date': 'Wed, 19 Mar 2025 14:07:21 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-groq-region': 'gcp-asia-south1', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '6000', 'x-ratelimit-remaining-requests': '14396', 'x-ratelimit-remaining-tokens': '4675', 'x-ratelimit-reset-requests': '23.984999999s', 'x-ratelimit-reset-tokens': '13.248s', 'x-request-id': 'req_01jpqbntkafvga1fwcq0zqjha5', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '922d8916acb30333-AMD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-03-19 19:37:20,665 - Status - INFO - LLM call
2025-03-19 19:37:20,685 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 19 Mar 2025 14:07:21 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'Vary', b'Origin'), (b'X-Groq-Region', b'gcp-asia-south1'), (b'X-Ratelimit-Limit-Requests', b'14400'), (b'X-Ratelimit-Limit-Tokens', b'6000'), (b'X-Ratelimit-Remaining-Requests', b'14396'), (b'X-Ratelimit-Remaining-Tokens', b'4680'), (b'X-Ratelimit-Reset-Requests', b'23.983999999s'), (b'X-Ratelimit-Reset-Tokens', b'13.198s'), (b'X-Request-Id', b'req_01jpqbntkbfvg9rv1s2swejhk9'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'922d8916ad49032b-AMD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-03-19 19:37:20,690 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-03-19 19:37:20,690 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-03-19 19:37:20,692 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-03-19 19:37:20,692 - httpcore.http11 - DEBUG - response_closed.started
2025-03-19 19:37:20,692 - httpcore.http11 - DEBUG - response_closed.complete
2025-03-19 19:37:20,692 - groq._base_client - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "200 OK" Headers({'date': 'Wed, 19 Mar 2025 14:07:21 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-groq-region': 'gcp-asia-south1', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '6000', 'x-ratelimit-remaining-requests': '14396', 'x-ratelimit-remaining-tokens': '4680', 'x-ratelimit-reset-requests': '23.983999999s', 'x-ratelimit-reset-tokens': '13.198s', 'x-request-id': 'req_01jpqbntkbfvg9rv1s2swejhk9', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '922d8916ad49032b-AMD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-03-19 19:37:20,699 - Status - INFO - LLM call
2025-03-19 19:37:20,776 - Status - INFO - Final Output
2025-03-19 19:37:21,756 - urllib3.connectionpool - DEBUG - https://api.smith.langchain.com:443 "POST /runs/multipart HTTP/1.1" 202 34
2025-03-19 19:37:23,813 - groq._base_client - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'Analyze the sentement of user feedback, and answer if its postive(meaning to further improvement required)or else negative'}, {'role': 'user', 'content': 'This is the user feedback: '}], 'model': 'llama3-70b-8192', 'n': 1, 'stop': None, 'stream': False, 'temperature': 0.7, 'tool_choice': {'type': 'function', 'function': {'name': 'Sentement'}}, 'tools': [{'type': 'function', 'function': {'name': 'Sentement', 'description': "dict() -> new empty dictionary\ndict(mapping) -> new dictionary initialized from a mapping object's\n    (key, value) pairs\ndict(iterable) -> new dictionary initialized as if via:\n    d = {}\n    for k, v in iterable:\n        d[k] = v\ndict(**kwargs) -> new dictionary initialized with the name=value pairs\n    in the keyword argument list.  For example:  dict(one=1, two=2)", 'parameters': {'type': 'object', 'properties': {'feedback': {'enum': ['Positive', 'Negative'], 'type': 'string'}}, 'required': ['feedback']}}}]}}
2025-03-19 19:37:23,813 - groq._base_client - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2025-03-19 19:37:23,829 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-03-19 19:37:23,829 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-03-19 19:37:23,829 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-03-19 19:37:23,831 - httpcore.http11 - DEBUG - send_request_body.complete
2025-03-19 19:37:23,831 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-03-19 19:37:24,126 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 19 Mar 2025 14:07:24 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'Vary', b'Origin'), (b'X-Groq-Region', b'gcp-asia-south1'), (b'X-Ratelimit-Limit-Requests', b'14400'), (b'X-Ratelimit-Limit-Tokens', b'6000'), (b'X-Ratelimit-Remaining-Requests', b'14392'), (b'X-Ratelimit-Remaining-Tokens', b'2280'), (b'X-Ratelimit-Reset-Requests', b'43.505s'), (b'X-Ratelimit-Reset-Tokens', b'37.191s'), (b'X-Request-Id', b'req_01jpqbnz09fvgs3ajv57yab1nv'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'922d8932fba6032c-AMD'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-03-19 19:37:24,126 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-03-19 19:37:24,126 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-03-19 19:37:24,126 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-03-19 19:37:24,126 - httpcore.http11 - DEBUG - response_closed.started
2025-03-19 19:37:24,126 - httpcore.http11 - DEBUG - response_closed.complete
2025-03-19 19:37:24,126 - groq._base_client - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "200 OK" Headers({'date': 'Wed, 19 Mar 2025 14:07:24 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-groq-region': 'gcp-asia-south1', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '6000', 'x-ratelimit-remaining-requests': '14392', 'x-ratelimit-remaining-tokens': '2280', 'x-ratelimit-reset-requests': '43.505s', 'x-ratelimit-reset-tokens': '37.191s', 'x-request-id': 'req_01jpqbnz09fvgs3ajv57yab1nv', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '922d8932fba6032c-AMD', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-03-19 19:37:24,126 - Status - INFO - User Feedback - NoImprovement
2025-03-19 19:37:24,126 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): mermaid.ink:443
2025-03-19 19:37:25,054 - urllib3.connectionpool - DEBUG - https://api.smith.langchain.com:443 "POST /runs/multipart HTTP/1.1" 202 34
2025-03-19 19:37:25,636 - urllib3.connectionpool - DEBUG - https://mermaid.ink:443 "GET /img/JSV7aW5pdDogeydmbG93Y2hhcnQnOiB7J2N1cnZlJzogJ2xpbmVhcid9fX0lJQpncmFwaCBURDsKCV9fc3RhcnRfXyhbPHA+X19zdGFydF9fPC9wPl0pOjo6Zmlyc3QKCVdvcmtfTWFuYWdlcihXb3JrIE1hbmFnZXIpCglQcmFsbGVsX1dva2VycyhQcmFsbGVsIFdva2VycykKCUNvbWJpbmVyKENvbWJpbmVyKQoJdG9vbF9sb2FkZXIodG9vbF9sb2FkZXIpCglfX2VuZF9fKFs8cD5fX2VuZF9fPC9wPl0pOjo6bGFzdAoJUHJhbGxlbF9Xb2tlcnMgLS0+IENvbWJpbmVyOwoJX19zdGFydF9fIC0tPiB0b29sX2xvYWRlcjsKCXRvb2xfbG9hZGVyIC0tPiBXb3JrX01hbmFnZXI7CglXb3JrX01hbmFnZXIgLS4tPiBQcmFsbGVsX1dva2VyczsKCUNvbWJpbmVyIC0uICZuYnNwO1Bhc3MmbmJzcDsgLi0+IF9fZW5kX187CglDb21iaW5lciAtLiAmbmJzcDtQb2xpc2gmbmJzcDsgLi0+IFdvcmtfTWFuYWdlcjsKCWNsYXNzRGVmIGRlZmF1bHQgZmlsbDojZjJmMGZmLGxpbmUtaGVpZ2h0OjEuMgoJY2xhc3NEZWYgZmlyc3QgZmlsbC1vcGFjaXR5OjAKCWNsYXNzRGVmIGxhc3QgZmlsbDojYmZiNmZjCg==?type=png&bgColor=!white HTTP/1.1" 200 18793
2025-03-19 19:37:25,652 - PIL.PngImagePlugin - DEBUG - STREAM b'IHDR' 16 13
2025-03-19 19:37:25,652 - PIL.PngImagePlugin - DEBUG - STREAM b'sRGB' 41 1
2025-03-19 19:37:25,652 - PIL.PngImagePlugin - DEBUG - STREAM b'IDAT' 54 8192
2025-03-19 19:37:25,824 - langsmith.client - DEBUG - Closing Client.session
2025-03-19 19:37:25,824 - langsmith.client - DEBUG - Closing Client.session
